\chapter{Related Work}

In previous chapters, we presented the SableWasm and evaluated its performance
against several well-known benchmark suites. SableWasm is not the only
WebAssembly runtime environment system that allows sandboxed WebAssembly
modules to run outside of the browser. This chapter will provide a quick
overview of several existing WebAssembly host environment implementations. Also,
SableWasm does not implement any auto-vectorization algorithms and heavily
depends on LLVM, both in the frontend WASI-enabled Clang compiler and the
SableWasm backend, to generate parallel code. Auto-vectorization is one of the
key research fields in compiler optimization, and much research has been
devoted to the field. Therefore, we will briefly cover auto-vectorization
in LLVM in this chapter.

\section*{WebAssembly runtime environments}

In chapter~\ref{chapter:evaluation}, we mentioned two WebAssembly runtime
environments developed by the community, Wasmtime and Wasmer. Wasmtime perhaps
is the earliest non-browser WebAssembly runtime environment. It started as a
side project during WebAssembly standardization and is maintained by Bytecode
Alliance\footnote{Bytecode Alliance: \url{https://bytecodealliance.org/}}. This
cross-industry nonprofit organization focuses on extending WebAssembly and WASI
beyond the browser and IoT devices. Wasmtime is built on the Cranelift compiler
framework
\footnote{Cranelift:\url{https://github.com/bytecodealliance/cranelift}}.
Cranelift is similar to LLVM, providing a target-independent intermediate
representation that eventually translates to native executable machine code.
Currently, at the time of thesis writing, Cranelift is still at very early
stages and only supports the x86-64 target. Although the Cranelift started as
the backend for Wasmtime, it is not limited to the Wasmtime project. In the
future, Cranelift may replace the fast debug backend in the Rust compiler
toolchain and the Javascript/WebAssembly engine backend in SpiderMonkey.

Wasmer \footnote{Wasmer:\url{https://wasmer.io/}} is another WebAssembly
runtime environment and maintained by a startup company. Wasmer shares many
similarities comparing to Wasmtime. However, it is more flexible in design.
Currently, Wasmer has three different backends, LLVM, Cranelift, and a
single-pass compiler for fast code generation. Additionally, comparing to
Wasmtime, Wasmer is more aggressive in adding features to WebAssembly. For
example, Wasmtime only supports WASI as the system interface API, while Wasmer
supports both the WASI and Emscripten specifications. Wasmer also comes with a
package manager, called WebAssembly Package Manager (WAPM)
\footnote{WebAssembly Package Manager (WAPM): \url{https://wapm.io/}} which
distributed pre-compiled sandboxed WebAssembly binary modules for various
applications.

Wasmtime and Wasmer are both just-in-time (JIT) WebAssembly runtime
environments. There are also ahead-of-time (AOT) compilers for WebAssembly
modules. The most notable one is perhaps the Lucet compiler.
Lucet\footnote{Lucet: \url{https://www.fastlylabs.com/}} is developed by
Fastly and shares a similar design to SableWasm. The initial motivation for
Lucet is to create a cloud application system that hosts user-uploaded
WebAssembly modules. Currently, Lucet powers Fastly's Terrarium platform,
an in-browser multi-language IDE. The Lucet compiler system has two parts,
the Lucet shared library compiler, and the Lucet shared library loader.
The Lucet shared library compiler compiles WebAssembly modules into shared
libraries, while Lucet shared library load dynamically loads the shared
library and executes the entry function, \texttt{\_start}. Unlike SableWasm,
Lucet is also built on the Cranelift compile framework.

All the WebAssembly environments we have discussed in the section are built
on complex compiler frameworks such as Cranelift or LLVM. Therefore, one
question that arises naturally is whether WebAssembly is suitable in a
resource-constrained environment such as an embedded system. Scheidl shows that
it is possible to translate WebAssembly bytecode, under these conditions, into
native executable code while maintaining decent performance
\cite{webassembly-embedded}. One interesting application for WebAssembly is to
use it as a form of distributing programs on IoT devices. For example,
Jacobsson and Will√©n implement a WebAssembly interpreter on an SoC that
communicates and receives modules from a host device
\cite{webassembly-wearables}. The system runs on low-power Bluetooth, and in
theory, can be used on a wearable device. Another WebAssembly runtime system,
Twine, presented in paper \cite{webassembly-sgx}, focuses on taking advantage
of hardware features to further improve the performance of WebAssembly in
trusted execution environments (TEE). For example, Twine takes advantage of the
Intel SGX instruction set to ensure the module's security and achieve up to
4.1x speedup in performance. This, of course, comes with the drawback of
additional hardware-specific dependencies.

\section*{Auto-vectorization}

This section will briefly discuss auto-vectorization in compilers, more
specifically, the LLVM compiler framework. Modern CPU architectures support
vector operations to some degree, such as SSE \cite{sse-intel},
AVX \cite{avx-intel} on x86, and Neon \cite{arm-neon} on Arm. In recent years,
scalable vector extensions, such as Arm's SVE \cite{arm-sve}, offer even more
flexibility on vector size. Although these SIMD instruction set extensions
speed up the resulting program, programmers need to have an in-depth
understanding of the hardware system to handle them correctly through inlined
assembly or intrinsic functions. Additionally, these methods are highly
hardware-specific and cause troubles when porting programs to another platform.
Another approach to the problem is to ask the compiler to generate vectorized
code from traditional scalar code, hence the name auto-vectorization, which
is implemented in many modern compiler systems, such as GCC \cite{auto-vec-gcc}
and LLVM \footnote{Auto-vectorization in LLVM:
    \url{https://llvm.org/docs/Vectorizers.html}}. Here we will take the
LLVM auto-vectorizer as an example.

The first attempt for auto-vectorization in LLVM is the basic block vectorizer.
It works with a single basic block at a time and searches for common patterns.
If it finds any optimization opportunity, it will rewrite the basic block into
parallel form. One might notice that the basic block vectorizer has no
understanding of a loop structure and only perform auto-vectorization if and
only if the operations are already unrolled. To address this problem, the
second generation of auto-vectorizer is a single block loop vectorizer. The
single block loop vectorizer can recognize simple loop structures and consists
of two parts the loop legalizer and the loop transformer. The loop legalizer
determines if a loop structure can undergo auto-vectorization, and if so, the
loop transformer will perform the rewrite. The single block loop auto-vectorizer
can also perform loop unrolling if the induction variables are detected.
However, this sometimes leads to very aggressive optimization, which slows down
the generated code. Hence, in late 2012, the LLVM developers extended the
auto-vectorizer with a cost model \cite{llvm-vec-cost, llvm-vec-cost-avx}. The
cost model will determine whether a potential optimization worth it based on the
instruction set available on the target hardware and data dependency between
operands.

LLVM also performs another type of auto-vectorization called superword-level
parallelism (SLP) auto-vectorization \cite{slp-vectorization}. SLP
auto-vectorization combines similar
operations into vector operations, such as memory access and numerical
comparison. SLP auto-vectorization is similar to the basic block auto-vectorizer
discussed earlier in this section, except that it searches patterns in a
bottom-up fashion \footnote{
    \url{https://llvm.org/devmtg/2018-04/slides/Rocha-Look-Ahead\%20SLP.pdf}}.

Although auto-vectorization brings a silver lining to systemically transforming
scalar code into parallel form, it still suffers several drawbacks. The most
notable problem is that the dependency between instructions is usually not
apparent to the compiler, especially in a nested loop structure. Hence, the
compiler can only take a conservative approach when scheduling the program. One
possible solution is to employ a polyhedral model \cite{polyhedral} to analyze
the data dependency among variables. The polyhedral analysis creates polyhedra
based on the program and applies affine transformations to improve instruction
scheduling incrementally. LLVM implements the polyhedral analysis in the
project Polly \cite{polly} \footnote{LLVM Polly: \url{https://polly.llvm.org/}},
which can be used as a compiler plugin and generates scheduling and scope
information for instructions. Later, the LLVM loop auto-vectorizer can take
advantage of this information to provide better cost estimation. Recent work on
auto-vectorization has also exported the use of machine-learning algorithms
to make better decisions on cost versus benefit than can be done by simple cost
models \cite{ml-vectorization}.